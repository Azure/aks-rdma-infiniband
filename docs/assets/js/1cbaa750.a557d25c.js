"use strict";(self.webpackChunkaks_rdma_infiniband=self.webpackChunkaks_rdma_infiniband||[]).push([[439],{4903:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"getting-started/introduction","title":"Introduction","description":"This documentation serves as a guide for deploying high-performance computing (HPC) workloads on Azure Kubernetes Service (AKS) clusters with support for Remote Direct Memory Access (RDMA) over InfiniBand (IB), and GPUDirect RDMA.","source":"@site/docs/getting-started/01-introduction.md","sourceDirName":"getting-started","slug":"/","permalink":"/aks-rdma-infiniband/","draft":false,"unlisted":false,"editUrl":"https://github.com/Azure/aks-rdma-infiniband/blob/main/website/docs/getting-started/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction","slug":"/"},"sidebar":"sidebar","next":{"title":"Prerequisites","permalink":"/aks-rdma-infiniband/getting-started/prerequisites"}}');var r=t(4848),o=t(8453);const s={title:"Introduction",slug:"/"},a=void 0,c={},d=[{value:"Core Components",id:"core-components",level:2},{value:"Beyond AKS",id:"beyond-aks",level:2},{value:"Support",id:"support",level:2}];function l(e){const n={a:"a",code:"code",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["This documentation serves as a guide for deploying high-performance computing (HPC) workloads on Azure Kubernetes Service (AKS) clusters with support for ",(0,r.jsx)(n.strong,{children:"Remote Direct Memory Access (RDMA) over InfiniBand (IB)"}),", and ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/gpudirect",children:"GPUDirect RDMA"})}),"."]}),"\n",(0,r.jsx)(n.p,{children:"HPC workloads, including distributed AI model training and serving require efficient, low-latency communication between nodes. Traditional networking protocols, such as TCP/IP, often introduce performance overheads that limit scalability. RDMA over InfiniBand mitigates these constraints by enabling direct memory access between devices, bypassing CPU and kernel. When combined with NVIDIA\u2019s GPUDirect RDMA, it facilitates direct GPU-to-GPU communication across nodes, optimizing throughput and latency for GPU-accelerated applications."}),"\n",(0,r.jsx)(n.h2,{id:"core-components",children:"Core Components"}),"\n",(0,r.jsxs)(n.p,{children:["This guide focuses on two NVIDIA operators that enable these functionalities within AKS clusters: the NVIDIA ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/networking/display/cokan10/network+operator",children:"Network Operator"})," and the NVIDIA ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html",children:"GPU Operator"}),". To streamline deployment and ensure optimal performance, this documentation provides recommended configurations and Helm values for both operators, tailored to Azure\u2019s high-performance infrastructure."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\ud83c\udf10 ",(0,r.jsx)(n.strong,{children:"Network Operator"}),": Automates the deployment and management of networking components, including Mellanox NICs and drivers, to support RDMA over Infiniband."]}),"\n",(0,r.jsxs)(n.li,{children:["\ud83d\udda5\ufe0f ",(0,r.jsx)(n.strong,{children:"GPU Operator"}),": Facilitates the deployment and management of NVIDIA GPU drivers, container runtimes, and Kubernetes device plugins, ensuring that GPUs are correctly configured for pods. The official ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/microsoft-aks.html",children:"GPU Operator documentation on AKS"})," is a good starting point for understanding the GPU Operator's role in AKS clusters."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["These operators integrate with Azure\u2019s HPC virtual machine offerings (e.g., ",(0,r.jsx)(n.code,{children:"NDasrA100_v4"}),", ",(0,r.jsx)(n.code,{children:"NDm_A100_v4"}),") and Mellanox ConnectX NICs to provide a solution for deploying and managing HPC workloads on AKS."]}),"\n",(0,r.jsx)(n.h2,{id:"beyond-aks",children:"Beyond AKS"}),"\n",(0,r.jsxs)(n.p,{children:["While this guide is designed specifically for AKS, the underlying concepts, configurations, and best practices for RDMA over InfiniBand, and GPUDirect RDMA can be adapted to other Kubernetes clusters hosted on Azure. For example, clusters managed via ",(0,r.jsx)(n.a,{href:"https://capz.sigs.k8s.io/",children:"Cluster API for Azure (CAPZ)"})," can leverage these principles with appropriate modifications to account for differences in cluster provisioning and management."]}),"\n",(0,r.jsx)(n.h2,{id:"support",children:"Support"}),"\n",(0,r.jsxs)(n.p,{children:["This guide is an open-source project hosted at ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/aks-rdma-infiniband",children:"github.com/Azure/aks-rdma-infiniband"})," and is not covered by the ",(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/troubleshoot/azure/virtual-machines/linux/support-linux-open-source-technology",children:"Microsoft Azure Support Policy"}),". For assistance, please review ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/aks-rdma-infiniband/issues",children:"existing issues"})," on the project repository. If your question or issue is not addressed, you are encouraged to ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/aks-rdma-infiniband/issues/new",children:"submit a new issue"}),". The project maintainers will respond to the best of their abilities."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(6540);const r={},o=i.createContext(r);function s(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);