"use strict";(globalThis.webpackChunkaks_rdma_infiniband=globalThis.webpackChunkaks_rdma_infiniband||[]).push([[820],{6864:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"configurations/gpu-drivers","title":"GPU Drivers","description":"This guide details recommended configurations to enable GPU drivers with specific settings for GPUDirect RDMA integration. Users can choose between AKS-managed GPU drivers or the NVIDIA GPU Operator for driver lifecycle management. The guide also provides examples of how to configure pods to utilize GPUDirect RDMA with different device plugin options.","source":"@site/docs/configurations/02-gpu-drivers.md","sourceDirName":"configurations","slug":"/configurations/gpu-drivers","permalink":"/aks-rdma-infiniband/configurations/gpu-drivers","draft":false,"unlisted":false,"editUrl":"https://github.com/Azure/aks-rdma-infiniband/blob/main/website/docs/configurations/02-gpu-drivers.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"GPU Drivers"},"sidebar":"sidebar","previous":{"title":"Network Operator","permalink":"/aks-rdma-infiniband/configurations/network-operator"}}');var t=i(4848),o=i(8453);const a={title:"GPU Drivers"},s=void 0,d={},l=[{value:"GPU Drivers: AKS-managed vs. GPU Operator-managed",id:"gpu-drivers-aks-managed-vs-gpu-operator-managed",level:2},{value:"Option 1: AKS-managed GPU Driver",id:"option-1-aks-managed-gpu-driver",level:3},{value:"Option 2: GPU Operator Deployment",id:"option-2-gpu-operator-deployment",level:3},{value:"Usage of GPUDirect RDMA",id:"usage-of-gpudirect-rdma",level:2},{value:"1. SR-IOV Device Plugin",id:"1-sr-iov-device-plugin",level:3},{value:"2. RDMA Shared Device Plugin",id:"2-rdma-shared-device-plugin",level:3},{value:"3. IP over InfiniBand (IPoIB)",id:"3-ip-over-infiniband-ipoib",level:3},{value:"Order of Operations",id:"order-of-operations",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This guide details recommended configurations to enable GPU drivers with specific settings for GPUDirect RDMA integration. Users can choose between AKS-managed GPU drivers or the NVIDIA GPU Operator for driver lifecycle management. The guide also provides examples of how to configure pods to utilize GPUDirect RDMA with different device plugin options."}),"\n",(0,t.jsx)(n.h2,{id:"gpu-drivers-aks-managed-vs-gpu-operator-managed",children:"GPU Drivers: AKS-managed vs. GPU Operator-managed"}),"\n",(0,t.jsx)(n.p,{children:"When provisioning GPU nodepools in an AKS cluster, the cluster administrator has the option to either rely on the default GPU driver installation managed by AKS or via GPU Operator. This decision impacts cluster setup, maintenance, and compatibility."}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"AKS-managed GPU Driver (Without GPU Operator)"})}),(0,t.jsx)(n.th,{children:(0,t.jsxs)(n.strong,{children:["GPU Operator-managed GPU Driver (",(0,t.jsx)(n.code,{children:"--gpu-driver none"}),")"]})})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Automation"})}),(0,t.jsx)(n.td,{children:"AKS-managed drivers; cluster administrator needs to manually deploy device plugins"}),(0,t.jsx)(n.td,{children:"Automates installation of driver, device plugins, and container runtimes via GPU Operator"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Complexity"})}),(0,t.jsx)(n.td,{children:"Simple, no additional components except device plugins"}),(0,t.jsx)(n.td,{children:"More complex, requires GPU Operator and additional components"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Support"})}),(0,t.jsx)(n.td,{children:"Fully supported by AKS; no preview features"}),(0,t.jsx)(n.td,{children:"No AKS support; driver install and maintenance with self-managed NVIDIA GPU Operator"})]})]})]}),"\n",(0,t.jsx)(n.admonition,{type:"danger",children:(0,t.jsxs)(n.p,{children:["AKS-managed GPU drivers and the NVIDIA GPU Operator managed GPU drivers are ",(0,t.jsx)(n.strong,{children:"mutually exclusive"})," and cannot coexist. When you create a nodepool ",(0,t.jsx)(n.strong,{children:"without"})," setting the ",(0,t.jsx)(n.code,{children:"--gpu-driver"})," field to ",(0,t.jsx)(n.code,{children:"none"}),", AKS provisions the nodepool with NVIDIA drivers and the NVIDIA container runtime. Installing GPU Operator subsequently replaces this setup by deploying its own ",(0,t.jsx)(n.code,{children:"nvidia-container-toolkit"}),", overriding the AKS-managed configuration. Upon uninstalling GPU Operator, the toolkit cannot revert to the original AKS containerd configuration, as it lacks awareness of the prior state, potentially disrupting the node\u2019s container runtime and impairing workload execution."]})}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Read more about the GPU driver installation options in AKS and the NVIDIA GPU Operator in the ",(0,t.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/aks/gpu-cluster?tabs=add-ubuntu-gpu-node-pool",children:"AKS documentation"})," and the ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/microsoft-aks.html",children:"GPU Operator documentation"}),"."]})}),"\n",(0,t.jsx)(n.h3,{id:"option-1-aks-managed-gpu-driver",children:"Option 1: AKS-managed GPU Driver"}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["Please proceed with GPU operator installation only if you have set the ",(0,t.jsx)(n.code,{children:"--gpu-driver"})," field to ",(0,t.jsx)(n.code,{children:"none"})," as described in ",(0,t.jsx)(n.a,{href:"/aks-rdma-infiniband/getting-started/prerequisites#aks-managed-gpu-driver",children:"prerequisites documentation"}),"."]})}),"\n",(0,t.jsxs)(n.p,{children:["To enable GPUDirect RDMA, the ",(0,t.jsx)(n.code,{children:"nvidia-peermem"})," kernel module must be loaded on the GPU nodes. The AKS-managed GPU driver installation does not load the Nvidia peer memory kernel module automatically. To ensure that this module is loaded on all GPU nodes, run the following command:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -k https://github.com/Azure/aks-rdma-infiniband/configs/nvidia-peermem-reloader\n"})}),"\n",(0,t.jsx)(n.h3,{id:"option-2-gpu-operator-deployment",children:"Option 2: GPU Operator Deployment"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["This section assumes a basic understanding of GPU Operator and its role in Kubernetes clusters. Readers unfamiliar with GPU Operator are advised to review the official ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html",children:"guide"})," before proceeding. The concepts and recommended configurations presented here build on that foundation to enable GPU workload and GPUDirect RDMA in AKS."]})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsxs)(n.p,{children:["Please proceed with GPU operator installation only if you have created the nodepool ",(0,t.jsx)(n.strong,{children:"with"})," the ",(0,t.jsx)(n.code,{children:"--gpu-driver"})," field set to ",(0,t.jsx)(n.code,{children:"none"})," as described in ",(0,t.jsx)(n.a,{href:"/aks-rdma-infiniband/getting-started/prerequisites#gpu-operator-managed-gpu-driver",children:"prerequisites documentation"}),"."]})}),"\n",(0,t.jsxs)(n.p,{children:["GPU Operator is deployed using ",(0,t.jsx)(n.a,{href:"https://helm.sh/",children:"Helm"}),", and the ",(0,t.jsx)(n.a,{href:"https://github.com/NVIDIA/gpu-operator/blob/v25.3.4/deployments/gpu-operator/values.yaml",children:"default Helm values"})," are customized to align with the Network Operator and AKS requirements. Key adjustments to the Helm values disable redundant components such as NFD and enable RDMA support."]}),"\n",(0,t.jsxs)(n.p,{children:["GPU operator deploys pods that require privileged access to the host system. To ensure proper operation, the ",(0,t.jsx)(n.code,{children:"gpu-operator"})," namespace must be labeled with ",(0,t.jsx)(n.code,{children:"pod-security.kubernetes.io/enforce=privileged"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl create ns gpu-operator\nkubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce=privileged\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Save the following YAML configuration to a file named ",(0,t.jsx)(n.code,{children:"values.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",metastring:"reference",children:"https://github.com/Azure/aks-rdma-infiniband/blob/main/configs/values/gpu-operator/values.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"Deploy GPU Operator with the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\nhelm repo update\n\nhelm upgrade --install \\\n  --create-namespace -n gpu-operator \\\n  gpu-operator nvidia/gpu-operator \\\n  -f values.yaml \\\n  --version v25.3.4\n"})}),"\n",(0,t.jsx)(n.h2,{id:"usage-of-gpudirect-rdma",children:"Usage of GPUDirect RDMA"}),"\n",(0,t.jsxs)(n.p,{children:["Once GPU Operator and its operands are installed, configure pods to claim both GPUs and InfiniBand resources created from one of the ",(0,t.jsx)(n.a,{href:"network-operator#nicclusterpolicy",children:"device plugins managed via Network Operator"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"1-sr-iov-device-plugin",children:"1. SR-IOV Device Plugin"}),"\n",(0,t.jsxs)(n.p,{children:["Here is an example for a GPUDirect RDMA workload using ",(0,t.jsx)(n.a,{href:"network-operator#1-sr-iov-device-plugin",children:"SR-IOV Device Plugin"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpudirect-rdma\nspec:\n  containers:\n  - name: gpudirect-rdma\n    image: images.my-company.example/app:v4\n    securityContext:\n      capabilities:\n        # A pod without this will have a low locked memory value `# ulimit\n        # -l` value of "64", this changes the value to "unlimited".\n        add: ["IPC_LOCK"]\n    resources:\n      requests:\n        nvidia.com/gpu: 8 # Claims all GPUs on the node\n        rdma/ib: 8        # Claims 8 NIC; adjust to match node\u2019s NIC count\n      limits:\n        nvidia.com/gpu: 8\n        rdma/ib: 8\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-rdma-shared-device-plugin",children:"2. RDMA Shared Device Plugin"}),"\n",(0,t.jsxs)(n.p,{children:["Here is an example for a GPUDirect RDMA workload using ",(0,t.jsx)(n.a,{href:"network-operator#2-rdma-shared-device-plugin",children:"RDMA Shared Device Plugin"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpudirect-rdma\nspec:\n  containers:\n  - name: gpudirect-rdma\n    image: images.my-company.example/app:v4\n    securityContext:\n      capabilities:\n        # A pod without this will have a low locked memory value `# ulimit\n        # -l` value of "64", this changes the value to "unlimited".\n        add: ["IPC_LOCK"]\n    resources:\n      requests:\n        nvidia.com/gpu: 8 # Claims all GPUs on the node\n        rdma/shared_ib: 1 # Claims 1 of 63 pod slots; all NICs accessible\n      limits:\n        nvidia.com/gpu: 8\n        rdma/shared_ib: 1\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-ip-over-infiniband-ipoib",children:"3. IP over InfiniBand (IPoIB)"}),"\n",(0,t.jsxs)(n.p,{children:["Here is an example for a GPUDirect RDMA workload using ",(0,t.jsx)(n.a,{href:"network-operator#3-ip-over-infiniband-ipoib",children:"IPoIB"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Pod\nmetadata:\n  name: ib-pod\n  annotations:\n    # This name should match the IPoIBNetwork object we created earlier.\n    # You can find this config by running `kubectl get IPoIBNetwork`.\n    k8s.v1.cni.cncf.io/networks: aks-infiniband\nspec:\n  containers:\n  - name: ib\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        nvidia.com/gpu: 8 # Claims all GPUs on the node\n      limits:\n        nvidia.com/gpu: 8\n"})}),"\n",(0,t.jsx)(n.h2,{id:"order-of-operations",children:"Order of Operations"}),"\n",(0,t.jsx)(n.p,{children:"The installation process follows this sequence:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Operator Deployment"}),": The Helm chart installs GPU Operator, including its controller manager deployment to manage ",(0,t.jsx)(n.code,{children:"ClusterPolicy"})," reconciliation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"ClusterPolicy"})," Reconciliation"]}),": The GPU Operator controller manager reconciles ",(0,t.jsx)(n.code,{children:"ClusterPolicy"}),", a custom resource that defines the desired state of GPU Operator and its components. The operator continuously monitors the cluster for changes and ensures that the actual state matches the desired state defined in the ",(0,t.jsx)(n.code,{children:"ClusterPolicy"}),". The operator creates the following notable DaemonSets:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"nvidia-driver-daemonset"}),": Installs NVIDIA drivers on GPU nodes, blocking other components until complete, as the container runtime depends on it."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"nvidia-container-toolkit-daemonset"}),": Configures containerd with ",(0,t.jsx)(n.code,{children:"nvidia-container-runtime"})," as the default container runtime for creating containers going foward. Creates ",(0,t.jsx)(n.code,{children:"nvidia"})," ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/containers/runtime-class/",children:"RuntimeClass"}),", enabling subsequent deployments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"nvidia-device-plugin-daemonset"}),": Registers GPUs as claimable node resources (",(0,t.jsx)(n.code,{children:"nvidia.com/gpu"}),") via the ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/",children:"Device Plugin"})," framework."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"nvidia-dcgm-exporter"}),": Exports GPU telemetry as Prometheus metrics, enabling monitoring of GPU utilization and other metrics."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"nvidia-operator-validator"}),": Validates GPU Operator installation and configuration, ensuring that all components are functioning correctly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"gpu-feature-discovery"})," (GFD): Discovers GPU features and labels nodes with GPU attributes (e.g., ",(0,t.jsx)(n.code,{children:"nvidia.com/gpu.product=NVIDIA-A100-SXM4-40G"}),") for scheduling."]}),"\n"]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var r=i(6540);const t={},o=r.createContext(t);function a(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);