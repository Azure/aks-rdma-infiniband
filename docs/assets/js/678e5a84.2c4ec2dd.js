"use strict";(self.webpackChunkaks_rdma_infiniband=self.webpackChunkaks_rdma_infiniband||[]).push([[249],{8041:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"configurations/network-operator","title":"Network Operator","description":"This guide details recommended configurations for Network Operator v25.1.0 to enable RDMA over InfiniBand, optimized for AKS environments with Mellanox NICs.","source":"@site/docs/configurations/01-network-operator.md","sourceDirName":"configurations","slug":"/configurations/network-operator","permalink":"/aks-rdma-infiniband/configurations/network-operator","draft":false,"unlisted":false,"editUrl":"https://github.com/Azure/aks-rdma-infiniband/blob/main/website/docs/configurations/01-network-operator.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Network Operator"},"sidebar":"sidebar","previous":{"title":"Prerequisites","permalink":"/aks-rdma-infiniband/getting-started/prerequisites"},"next":{"title":"GPU Drivers","permalink":"/aks-rdma-infiniband/configurations/gpu-drivers"}}');var r=i(4848),s=i(8453);const a={title:"Network Operator"},t=void 0,l={},d=[{value:"Network Operator Deployment",id:"network-operator-deployment",level:2},{value:"Operator",id:"operator",level:3},{value:"Node Feature Rule",id:"node-feature-rule",level:3},{value:"NicClusterPolicy",id:"nicclusterpolicy",level:2},{value:"1. SR-IOV Device Plugin",id:"1-sr-iov-device-plugin",level:3},{value:"2. RDMA Shared Device Plugin",id:"2-rdma-shared-device-plugin",level:3},{value:"3. IP over InfiniBand (IPoIB)",id:"3-ip-over-infiniband-ipoib",level:3},{value:"4. Only Driver Installation",id:"4-only-driver-installation",level:3},{value:"Recommendations",id:"recommendations",level:3},{value:"Order of Operations",id:"order-of-operations",level:2},{value:"Frequently Asked Questions",id:"frequently-asked-questions",level:2},{value:"Why is secondary IP address assignment not required for RDMA over InfiniBand in AKS?",id:"why-is-secondary-ip-address-assignment-not-required-for-rdma-over-infiniband-in-aks",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"This guide details recommended configurations for Network Operator v25.1.0 to enable RDMA over InfiniBand, optimized for AKS environments with Mellanox NICs."}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["This guide assumes a basic understanding of Network Operator and its role in Kubernetes clusters. Readers unfamiliar with the Network Operator are advised to review the official ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/networking/display/kubernetes2501/getting-started-kubernetes.html",children:"Getting Started Guide"})," before proceeding. The concepts and recommended configurations presented here build on that foundation to enable RDMA over InfiniBand in AKS. This documentation is based on Network Operator v25.1.0."]})}),"\n",(0,r.jsx)(n.h2,{id:"network-operator-deployment",children:"Network Operator Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"operator",children:"Operator"}),"\n",(0,r.jsxs)(n.p,{children:["Network Operator is deployed using ",(0,r.jsx)(n.a,{href:"https://helm.sh/",children:"Helm"}),", and the ",(0,r.jsx)(n.a,{href:"https://github.com/Mellanox/network-operator/blob/v25.1.0/deployment/network-operator/values.yaml",children:"default Helm values"})," are recommended unless specific customizations are required. These defaults include ",(0,r.jsx)(n.a,{href:"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html",children:"Node Feature Discovery (NFD)"}),", a critical dependency that labels nodes with hardware details (e.g., Mellanox NIC presence) for pod scheduling."]}),"\n",(0,r.jsxs)(n.p,{children:["Network operator deploys pods that require privileged access to the host system. To ensure proper operation, the ",(0,r.jsx)(n.code,{children:"network-operator"})," namespace must be labeled with ",(0,r.jsx)(n.code,{children:"pod-security.kubernetes.io/enforce=privileged"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl create ns network-operator\nkubectl label --overwrite ns network-operator pod-security.kubernetes.io/enforce=privileged\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Save the following YAML configuration to a file named ",(0,r.jsx)(n.code,{children:"values.yaml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",metastring:"reference",children:"https://github.com/Azure/aks-rdma-infiniband/blob/main/configs/values/network-operator/values.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"Deploy Network Operator with the following commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\nhelm repo update\n\nhelm upgrade --install \\\n  --create-namespace -n network-operator \\\n  network-operator nvidia/network-operator \\\n  -f values.yaml \\\n  --version v25.1.0\n"})}),"\n",(0,r.jsx)(n.h3,{id:"node-feature-rule",children:"Node Feature Rule"}),"\n",(0,r.jsx)(n.p,{children:"The Node Feature Discovery (NFD) component of the Network Operator is responsible for labeling nodes with hardware features. The default configuration includes a rule to label nodes with the presence of Mellanox NICs. This rule is essential for the proper functioning of the RDMA over InfiniBand setup."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",metastring:"reference",children:"https://github.com/azure/aks-rdma-infiniband/blob/main/tests/setup-infra/network-operator-nfd.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"To deploy the above configuration, run the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f https://raw.githubusercontent.com/azure/aks-rdma-infiniband/refs/heads/main/tests/setup-infra/network-operator-nfd.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Once above configuration takes effect, you can see that the nodes with Nvidia Mellanox NICs are labeled with label ",(0,r.jsx)(n.code,{children:"feature.node.kubernetes.io/pci-15b3.present: true"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl get nodes -l "feature.node.kubernetes.io/pci-15b3.present=true" -o wide\n'})}),"\n",(0,r.jsx)(n.h2,{id:"nicclusterpolicy",children:"NicClusterPolicy"}),"\n",(0,r.jsxs)(n.p,{children:["After installation of the network operator, create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," Custom Resource (CR) to define the desired state of networking components, such as Mellanox driver version and which device plugins to deploy."]}),"\n",(0,r.jsx)(n.h3,{id:"1-sr-iov-device-plugin",children:"1. SR-IOV Device Plugin"}),"\n",(0,r.jsxs)(n.p,{children:["The SR-IOV Device Plugin assigns each InfiniBand-enabled NIC (e.g., Mellanox ConnectX-6) to a single pod as a Kubernetes resource (",(0,r.jsx)(n.code,{children:"rdma/ib"}),"). The number of available resources matches the count of physical NICs on the node (e.g., 1 NIC = 1 resource), ideal for workloads requiring maximum performance and isolation."]}),"\n",(0,r.jsxs)(n.p,{children:["To deploy the above config, create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," CR with the following YAML:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -k https://github.com/Azure/aks-rdma-infiniband/configs/nicclusterpolicy/sriov-device-plugin\n"})}),"\n",(0,r.jsx)(n.p,{children:"Example pod configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ib-pod\nspec:\n  containers:\n  - name: ib\n    image: images.my-company.example/app:v4\n    securityContext:\n      capabilities:\n        # A pod without this will have a low locked memory value `# ulimit\n        # -l` value of "64", this changes the value to "unlimited".\n        add: ["IPC_LOCK"]\n    resources:\n      requests:\n        rdma/ib: 8 # Claims 8 NIC; adjust to match node\u2019s NIC count\n      limits:\n        rdma/ib: 8\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-rdma-shared-device-plugin",children:"2. RDMA Shared Device Plugin"}),"\n",(0,r.jsxs)(n.p,{children:["The RDMA Shared Device Plugin enables multiple pods to share all InfiniBand NICs on a node, exposed as ",(0,r.jsx)(n.code,{children:"rdma/shared_ib"}),". The resource count represents the maximum number of concurrent pods (default: 63 per node, configurable), not the NICs themselves, suiting resource-efficient workloads."]}),"\n",(0,r.jsxs)(n.p,{children:["To deploy the above config, create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," CR with the following YAML:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -k https://github.com/Azure/aks-rdma-infiniband/configs/nicclusterpolicy/rdma-shared-device-plugin\n"})}),"\n",(0,r.jsx)(n.p,{children:"Example pod configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: Pod\nmetadata:\n  name: ib-pod\nspec:\n  containers:\n  - name: ib\n    image: images.my-company.example/app:v4\n    securityContext:\n      capabilities:\n        # A pod without this will have a low locked memory value `# ulimit\n        # -l` value of "64", this changes the value to "unlimited".\n        add: ["IPC_LOCK"]\n    resources:\n      requests:\n        rdma/shared_ib: 1 # Claims 1 of 63 pod slots; all NICs accessible\n      limits:\n        rdma/shared_ib: 1\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-ip-over-infiniband-ipoib",children:"3. IP over InfiniBand (IPoIB)"}),"\n",(0,r.jsx)(n.p,{children:"IP over InfiniBand (IPoIB) is a network protocol that allows IP packets to be transmitted over InfiniBand networks. If the application is not enlightened to do RDMA out of the box, IPoIB can be used to enable IP-based communication over InfiniBand. This allows any off the shelf application to take advantage of the InfiniBand network."}),"\n",(0,r.jsxs)(n.p,{children:["In this case, each pod will be assigned a secondary IP address from the network subnet defined in ",(0,r.jsx)(n.a,{href:"https://github.com/Azure/aks-rdma-infiniband/blob/main/configs/nicclusterpolicy/ipoib/ipoib-network.yaml",children:"this configuration"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["To enable IPoIB, create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," CR with the following YAML:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -k https://github.com/Azure/aks-rdma-infiniband/configs/nicclusterpolicy/ipoib\n"})}),"\n",(0,r.jsx)(n.p,{children:"Example pod configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Pod\nmetadata:\n  name: ib-pod\n  annotations:\n    # This name should match the IPoIBNetwork object we created earlier.\n    # You can find this config by running `kubectl get IPoIBNetwork`.\n    k8s.v1.cni.cncf.io/networks: aks-infiniband\nspec:\n  containers:\n  - name: ib\n    image: images.my-company.example/app:v4\n"})}),"\n",(0,r.jsx)(n.h3,{id:"4-only-driver-installation",children:"4. Only Driver Installation"}),"\n",(0,r.jsxs)(n.p,{children:["You can also create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," CR that only installs the Mellanox OFED driver without any device plugin."]}),"\n",(0,r.jsx)(n.admonition,{type:"danger",children:(0,r.jsx)(n.p,{children:"This configuration is not a recommended configuration as it does not provide any resource management or scheduling capabilities. This is useful for testing purposes only. Also note that applications have to run as privileged containers to access the InfiniBand devices."})}),"\n",(0,r.jsxs)(n.p,{children:["To deploy just the driver installation, create a ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," CR with the following YAML:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -k https://github.com/Azure/aks-rdma-infiniband/configs/nicclusterpolicy/base\n"})}),"\n",(0,r.jsx)(n.p,{children:"Example pod configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Pod\nmetadata:\n  name: ib-pod\nspec:\n  containers:\n  - name: ib\n    image: images.my-company.example/app:v4\n    securityContext:\n      privileged: true\n"})}),"\n",(0,r.jsx)(n.h3,{id:"recommendations",children:"Recommendations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Maximum Performance"}),": Use the SR-IOV Device Plugin with one pod per node claiming all InfiniBand NICs (e.g., ",(0,r.jsx)(n.code,{children:"rdma/ib: <NIC count>"}),"), ensuring exclusive RDMA access for optimal throughput and isolation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Efficiency"}),": Use the RDMA Shared Device Plugin for multi-pod sharing. For GPUDirect RDMA workloads, note that if a pod claims ",(0,r.jsx)(n.code,{children:"rdma/shared_ib: 1"})," and all GPUs (e.g., 8 on ",(0,r.jsx)(n.code,{children:"Standard_ND96asr_v4"}),"), no additional pods of the same type can schedule on that node due to GPU exhaustion, despite remaining RDMA slots."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Off the shelf Applications"}),": Use IPoIB for applications not RDMA-aware. This allows IP-based communication over InfiniBand, enabling existing applications to leverage the InfiniBand network."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"order-of-operations",children:"Order of Operations"}),"\n",(0,r.jsx)(n.p,{children:"The installation process follows this sequence:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Network Operator Deployment"}),": The Helm chart installs the Network Operator, including its controller manager deployment to manage ",(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," reconciliation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Node Feature Discovery (NFD)"}),": Deployed as part of Network Operator Helm chart. The ",(0,r.jsx)(n.code,{children:"NodeFeatureRule"})," CR helps NFD to label nodes with hardware details (e.g., Mellanox NICs) for certain pods to select nodes with specific hardware features."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"NicClusterPolicy"})," Reconciliation"]}),": Creates DaemonSets based on the CR:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"mofed-ubuntu22.04-ds"}),": Installs kernel drivers (e.g., Mellanox OFED and InfiniBand drivers) to enable RDMA over InfiniBand capabilities on the nodes."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"device-plugin"}),": Installs the SR-IOV Device Plugin and/or RDMA Shared Device Plugin, depending on the selected configuration. This plugin exposes the NICs as claimable resources in Kubernetes using the ",(0,r.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/",children:"Device Plugin"})," framework."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"frequently-asked-questions",children:"Frequently Asked Questions"}),"\n",(0,r.jsx)(n.h3,{id:"why-is-secondary-ip-address-assignment-not-required-for-rdma-over-infiniband-in-aks",children:"Why is secondary IP address assignment not required for RDMA over InfiniBand in AKS?"}),"\n",(0,r.jsxs)(n.p,{children:["RDMA over InfiniBand operates below the TCP/IP stack, relying on direct memory access rather than IP-based networking. Tools like ",(0,r.jsx)(n.a,{href:"https://github.com/k8snetworkplumbingwg/multus-cni",children:"Multus"})," and ",(0,r.jsx)(n.a,{href:"https://github.com/k8snetworkplumbingwg/whereabouts",children:"whereabouts"})," for secondary network attachment and IPAM are not strictly required for RDMA over InfiniBand in AKS, as Device Plugins directly expose InfiniBand resources to pods."]}),"\n",(0,r.jsxs)(n.p,{children:["If you wish to operate in the TCP/IP stack over the InfiniBand network, refer to the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/networking/display/kubernetes2501/getting-started-kubernetes.html",children:"NVIDIA Getting Started Guide for Kubernetes"})," for detailed instructions."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>t});var o=i(6540);const r={},s=o.createContext(r);function a(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);